import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.utils import to_categorical

Datos = 'basededatos.xlsx'
df = pd.read_excel(Datos, engine='openpyxl')

X = df.iloc[:, :6].values
# Datos de entrada

y = df.iloc[:, 6].values
# Datos de salida (Covertura Nevulosa)

# Estandarizamos los datos
scaler = StandardScaler()
X = scaler.fit_transform(X)

#Convertimos las etiquetas a formato categórico
encoder = LabelEncoder()
y = encoder.fit_transform(y)
y = to_categorical(y, num_classes=8)

# Dividimos los datos en entrenamiento y prueba (80%, 20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=42)

# Creamos la red neuronal
modelo = tf.keras.Sequential([
tf.keras.layers.Dense(64, activation='relu', input_shape= (X_train.shape[1],)),
tf.keras.layers.Dense(32, activation='relu'),
tf.keras.layers.Dense(8, activation='softmax')])

# Compilamos el modelo
modelo.compile(optimizer='AdamW', loss='categorical_crossentropy',
metrics=['accuracy'])

# Entramos el modelo
modelo.fit(X_train, y_train, epochs=100, batch_size=16, validation_data=(X_test, y_test))

# Evaluamos el modelo
score = modelo.evaluate(X_test, y_test)
print(f"Pérdida: {score[0]}, Precisión: {score[1]}")

# Número de parámetros
# Parámetros = 6*64 + 64 + 64*32 + 32 + 32*8 + 8 = 2792
modelo.summary()
total_params = modelo.count_params()
print(f"El número de parámetros es: {total_params}")
